---
layout: 2021
---

# PhysioNet/CinC Challenge 2021 Results

This page contains the final scores for the 2021 PhysioNet/CinC Challenge: 
- __Official entries__: only Challenge metric scores ([12-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-12-lead.csv), [6-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-6-lead.csv), [4-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-4-lead.csv), [3-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-3-lead.csv), [2-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-2-lead.csv), and [all-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-All-lead.csv)) and all scores ([12-lead](Final_2021_Challenge_All_Scores_Official_Entries-12-lead.csv), [6-lead](Final_2021_Challenge_All_Scores_Official_Entries-6-lead.csv), [4-lead](Final_2021_Challenge_All_Scores_Official_Entries-4-lead.csv), [3-lead](Final_2021_Challenge_All_Scores_Official_Entries-3-lead.csv), and [2-lead](Final_2021_Challenge_All_Scores_Official_Entries-2-lead.csv))
- __Unofficial entries__: only Challenge metric scores ([12-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-12-lead.csv), [6-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-6-lead.csv), [4-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-4-lead.csv), [3-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-3-lead.csv), [2-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-2-lead.csv), and [all-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-All-lead.csv)) and all scores ([12-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-12-lead.csv), [6-lead](/Final_2021_Challenge_All_Scores_Unofficial_Entries-6-lead.csv), [4-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-4-lead.csv), [3-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-3-lead.csv), and [2-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-2-lead.csv))
- __Team eligibility__: [here](Summary_Information_2021_Challenge_Entries.csv) (determines whether entries are official or unofficial)
- __Per-class metrics__: for both official and unofficial entries ([12-lead](2021_Challenge_Final_test_score_metrics_per_class-12-lead.csv), [6-lead](2021_Challenge_Final_test_score_metrics_per_class-6-lead.csv), [4-lead](2021_Challenge_Final_test_score_metrics_per_class-3-lead.csv), and [2-lead](2021_Challenge_Final_test_score_metrics_per_class-2-lead.csv))

These tables contain scores for the 12-lead, 6-lead, 4-lead, 3-lead, and 2-lead versions of the hidden validation and test sets. The [2021 Challenge webpage](../) and [2021 Challenge paper](../papers/2021ChallengePaper2021.pdf) describes the [lead combinations](../#data) and the validation and test [data sources](../#data-sources). We also included an _all-lead_ score in the tables, which is computed as the mean of the 12-lead, 3-lead, and 2-lead scores.

We used the [Challenge scoring metric](../#scoring) that we implemented in the [evaluation code repository](https://github.com/physionetchallenges/evaluation-2021) to rank the official entries on the test set. We sorted the unofficial entries alphabetically by team name.

In these tables, you can find the following information:
- The only Challenge metric scores of the official entries ([12-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-12-lead.csv), [6-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-6-lead.csv), [4-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-4-lead.csv), [3-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-3-lead.csv), [2-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-2-lead.csv), and [all-lead](2021_Challenge_Test_Results_Leaderboard_For_Official_Entries-All-lead.csv)) and only Challenge metric scores of the unofficial entries ([12-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-12-lead.csv), [6-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-6-lead.csv), [4-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-4-lead.csv), [3-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-3-lead.csv), [2-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-2-lead.csv), and [all-lead](2021_Challenge_Test_Results_Leaderboard_For_Unofficial_Entries-All-lead.csv)) CSV files provide the Challenge scoring metric on the hidden datasets as well as the the run time for training the models and running the trained models. You can find the results of every lead combination of the 12-lead, 6-lead, 4-lead, 3-lead, 2-lead and all-lead (mean of the scores on the 12-lead, 3-lead, and 2-lead scores) scores in separate CSV files.
- The all scores of the official entries ([12-lead](Final_2021_Challenge_All_Scores_Official_Entries-12-lead.csv), [6-lead](Final_2021_Challenge_All_Scores_Official_Entries-6-lead.csv), [4-lead](Final_2021_Challenge_All_Scores_Official_Entries-4-lead.csv), [3-lead](Final_2021_Challenge_All_Scores_Official_Entries-3-lead.csv), and [2-lead](Final_2021_Challenge_All_Scores_Official_Entries-2-lead.csv)) and all scores of the unofficial entries ([12-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-12-lead.csv), [6-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-6-lead.csv), [4-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-4-lead.csv), [3-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-3-lead.csv), and [2-lead](Final_2021_Challenge_All_Scores_Unofficial_Entries-2-lead.csv)) CSV files provide more evaluation metrics: the area under the receiver operating characteristic curve (AUROC), area under the precision recall curve (AUPRC), _F_-measure, accuracy, and the Challenge scoring metric scores. The accuracy metric is the fraction of correctly diagnosed recordings, i.e., all classes for the recording are correct. There are separate CSV files for the 12-lead, 6-lead, 4-lead, 3-lead, and 2-lead scores.
- The [team eligibility table](Summary_Information_2021_Challenge_Entries.csv) contains information about all of the 2021 Challenge entries (official and unofficial entries), including satisfiaction of the [rules](../#rules) for rankings and prize eligibility.
- The per-class metrics ([12-lead](2021_Challenge_Final_test_score_metrics_per_class-12-lead.csv), [6-lead](2021_Challenge_Final_test_score_metrics_per_class-6-lead.csv), [4-lead](2021_Challenge_Final_test_score_metrics_per_class-4-lead.csv), [3-lead](2021_Challenge_Final_test_score_metrics_per_class-3-lead.csv), and [2-lead](2021_Challenge_Final_test_score_metrics_per_class-2-lead.csv)) CSV files provide evaluation metrics (AUROC, AUPRC and  _F_-measure) per-class on the hidden test dataset for all (official and unofficial) entries, sorted alphabetically. Please note that per-class metrics are undefined for classes without labels belonging to that class. There are separate CSV files for the 12-lead, 6-lead, 4-lead, 3-lead, and 2-lead metrics.

To refer to these tables in your publication, please cite the following papers:

[Reyna MA, Sadr N, Perez Alday EA, Gu A, Shah AJ, Robichaux C, Rad AB, Elola A, Seyedi S, Ansari S, Ghanbari H, Li Q, Sharma A, Clifford GD. Will Two Do? Varying Dimensions in Electrocardiography: The PhysioNet/Computing in Cardiology Challenge 2021. Computing in Cardiology 2021; 48: 1-4](https://www.cinc.org/archives/2021/pdf/CinC2021-134.pdf)

[Reyna MA, Sadr N, Perez Alday EA, Gu Annie, Shah AJ, Robichaux C, Rad AB, Elola A, Seyedi S, Ansari S, Ghanbari H, Li Q, Sharma A, Clifford GD. Issues in the automated classification of multilead ECGs using heterogeneous labels and populations. Preprint. 2021](2021ChallengePaperPMEA.pdf)

---

Supported by the [National Institute of Biomedical Imaging and Bioengineering (NIBIB)](https://www.nibib.nih.gov/) under NIH grant R01EB030362.

[Back](../)
